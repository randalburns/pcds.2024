{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce265c4c-a998-41f8-a413-2919cde7edc3",
   "metadata": {},
   "source": [
    "## PySpark Friends of Friends\n",
    "\n",
    "We are going to reproduce the Friends of Friends Hadoop example in Spark.\n",
    "First things first, let's run it as a map/reduce program in Spark.\n",
    "\n",
    "Node A with neighbors B and C propose candidate triples to it's neighbors\n",
    "* B A C to node B (A<C, else B C A)\n",
    "* C A B to node C (A<B else C B A)\n",
    "\n",
    "All triples will get two proposals from it's neighbors and reduce them. If there are two matching proposals, we have a triple.\n",
    "\n",
    "#### Map/Reduce Implementation\n",
    "\n",
    "The code below `lines_to_triples` implements the proposal to be run in `flatMap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb9eca2-47cd-4201-b6ed-6c4c8ce93d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def line_to_triples(line: str):\n",
    "    fids = np.array(line.split(), dtype=int)\n",
    "    ret = []\n",
    "    for i in range(1, len(fids) - 1):\n",
    "        for j in  range(i + 1, len(fids)):\n",
    "            source = fids[0]\n",
    "            fi, fj = fids[i], fids[j]\n",
    "            if source < fi:\n",
    "                ret.append([fj, source, fi])\n",
    "            else:\n",
    "                ret.append([fj, fi, source])\n",
    "            if source < fj:\n",
    "                ret.append([fi, source, fj])\n",
    "            else:\n",
    "                ret.append([fi, fj, source])\n",
    "    return ret    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b704dc7-6fbf-42a2-bc7c-7c7442c62f5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Simple test to show what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3741e37d-be14-4f54-a983-25001937827e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 1, 5],\n",
       " [5, 1, 8],\n",
       " [7, 1, 5],\n",
       " [5, 1, 7],\n",
       " [9, 1, 5],\n",
       " [5, 1, 9],\n",
       " [7, 1, 8],\n",
       " [8, 1, 7],\n",
       " [9, 1, 8],\n",
       " [8, 1, 9],\n",
       " [9, 1, 7],\n",
       " [7, 1, 9]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_to_triples (\"1 5 8 7 9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a99c61-970d-4cbf-9bb6-c6d9bd599b54",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "Now, complete the program. You will have to use the wordcount style `<triple>, 1` to get a simpler reducer to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1bedf80-4de4-4fe6-abe2-6db9e50498f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/11 17:24:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o23.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input Pattern file:/home/jupyteruser/ebook/examples/data/simple.input/* matches 0 files\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Input Pattern file:/home/jupyteruser/ebook/examples/data/simple.input/* matches 0 files\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:280)\n\t... 25 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m### TODO add a WordCount style reducer\u001b[39;00m\n\u001b[1;32m     11\u001b[0m rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: (\u001b[38;5;28mstr\u001b[39m(x), \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 12\u001b[0m rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mreduceByKey(\u001b[38;5;28;01mlambda\u001b[39;00m x, y: x \u001b[38;5;241m+\u001b[39m y)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m### TODO Identify triples of count >=2\u001b[39;00m\n\u001b[1;32m     14\u001b[0m rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/rdd.py:3552\u001b[0m, in \u001b[0;36mRDD.reduceByKey\u001b[0;34m(self, func, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   3505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduceByKey\u001b[39m(\n\u001b[1;32m   3506\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[Tuple[K, V]]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3507\u001b[0m     func: Callable[[V, V], V],\n\u001b[1;32m   3508\u001b[0m     numPartitions: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3509\u001b[0m     partitionFunc: Callable[[K], \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m portable_hash,\n\u001b[1;32m   3510\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[Tuple[K, V]]\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3511\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3512\u001b[0m \u001b[38;5;124;03m    Merge the values for each key using an associative and commutative reduce function.\u001b[39;00m\n\u001b[1;32m   3513\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3550\u001b[0m \u001b[38;5;124;03m    [('a', 2), ('b', 1)]\u001b[39;00m\n\u001b[1;32m   3551\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombineByKey(\u001b[38;5;28;01mlambda\u001b[39;00m x: x, func, func, numPartitions, partitionFunc)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/rdd.py:3975\u001b[0m, in \u001b[0;36mRDD.combineByKey\u001b[0;34m(self, createCombiner, mergeValue, mergeCombiners, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   3913\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3914\u001b[0m \u001b[38;5;124;03mGeneric function to combine the elements for each key using a custom\u001b[39;00m\n\u001b[1;32m   3915\u001b[0m \u001b[38;5;124;03mset of aggregation functions.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3972\u001b[0m \u001b[38;5;124;03m[('a', [1, 2]), ('b', [1])]\u001b[39;00m\n\u001b[1;32m   3973\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numPartitions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3975\u001b[0m     numPartitions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_defaultReducePartitions()\n\u001b[1;32m   3977\u001b[0m serializer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39mserializer\n\u001b[1;32m   3978\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_limit()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/rdd.py:4867\u001b[0m, in \u001b[0;36mRDD._defaultReducePartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39mdefaultParallelism\n\u001b[1;32m   4866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4867\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetNumPartitions()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/rdd.py:5453\u001b[0m, in \u001b[0;36mPipelinedRDD.getNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetNumPartitions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m-> 5453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd\u001b[38;5;241m.\u001b[39mpartitions()\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o23.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input Pattern file:/home/jupyteruser/ebook/examples/data/simple.input/* matches 0 files\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Input Pattern file:/home/jupyteruser/ebook/examples/data/simple.input/* matches 0 files\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:280)\n\t... 25 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../data/simple.input\"\n",
    "outdir = \"/tmp/outputsimple.mr49\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "rdd = rdd.flatMap(line_to_triples)\n",
    "### TODO add a WordCount style reducer\n",
    "rdd = rdd.map(lambda x: (str(x), 1))\n",
    "rdd = rdd.reduceByKey(lambda x, y: x + y)\n",
    "### TODO Identify triples of count >=2\n",
    "rdd = rdd.filter(lambda x: x[1]>1)\n",
    "### TODO format output to just triples\n",
    "rdd = rdd.map(lambda x: x[0])\n",
    "rdd.saveAsTextFile(outdir)\n",
    "\n",
    "sc.stop()             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1077687-b540-4481-8566-52082c7b4e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9824f04e-3c9c-4b23-9884-83babd85a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../data/simple.input\"\n",
    "outdir = \"/tmp/outputfof.mr5\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "rdd = rdd.flatMap(line_to_triples)\n",
    "rdd = rdd.map(lambda x: (str(x), 1))\n",
    "rdd = rdd.reduceByKey(lambda x, y: x + y)\n",
    "rdd = rdd.filter(lambda x: x[1] > 1)\n",
    "rdd = rdd.map(lambda x: x[0])\n",
    "rdd.saveAsTextFile(outdir)\n",
    "\n",
    "sc.stop()             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbdb03f-13e9-4038-a547-9e637d4659cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e81ac7-27ec-498f-ac24-c26dc4ee38f2",
   "metadata": {},
   "source": [
    "#### Join Implementation\n",
    "\n",
    "In previous years when we ran this on distributed memory, we found that an implementation using a `join` was faster. This is because join has efficient implementation (based on a hash map) that do not have to send all the triples around. The concept of the program is:\n",
    "* output all triples (same as before)\n",
    "* add an partition number to each triple\n",
    "* perform a self-join on the triples with partition numbers\n",
    "* use the partition index to identify when the triples came from different lists\n",
    "    * self-join will produce joined triples from the same partition \n",
    "* output triples when two proposals came from different partitions\n",
    "    * i.e. two proposal from different friends lists\n",
    "    * you need to avoid additional output when there is no match\n",
    "    \n",
    "This is a pretty different usage of Spark.  The function `mapPartitions` allows the programmer to write functions that apply to an entire partition, rather than each individual element of an RDD.  The function `mapPartitionsWithIndex` makes the partition identifier available to differentiate behavior, analagous to have the thread ID in OpenMP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e48f4-7be1-4e9e-8516-6c8e25b1073b",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following helper function will add an index to the triple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8a9865-d765-4935-ab73-a2d1aecef851",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### add a parition index to each triple.\n",
    "def add_index (idx, part):\n",
    "    for p in part:\n",
    "        yield str(p), str(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68537dc4-8182-4533-960d-ede8f5df6309",
   "metadata": {
    "tags": []
   },
   "source": [
    "You will need to write a function to identify when the joined triples have different indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7638ab0-0b93-4131-87e2-a2931749ba37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_diff_idx (x):\n",
    "    if x[1][0] != x[1][1]:\n",
    "        return x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33970c5-0107-45cc-b78f-fcd692bf4eba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../data/simple.input\"\n",
    "outdir = \"/tmp/output.join\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "rdd = rdd.flatMap(line_to_triples)\n",
    "rdd = rdd.mapPartitionsWithIndex(add_index)\n",
    "rdd = rdd.join(rdd)\n",
    "rdd = rdd.map(filter_diff_idx)\n",
    "rdd = rdd.filter(lambda x: x!= None)\n",
    "rdd.saveAsTextFile(outdir)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61753d5-ddd1-4dcc-915c-de554e0054a7",
   "metadata": {},
   "source": [
    "The following `sc.stop()` can be called alone to clean up crashed environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04ef2a6-420d-43d6-ae0c-734a324c03eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f482b323-0976-4e1b-b069-f60b7741a1b0",
   "metadata": {},
   "source": [
    "It turns out (we will see below) that this version is not faster on shared-memory. It does a lot more computation and the reduction in traffic is not as important, because memory is faster than networking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3871b123-cd3b-4619-8f8e-2af4cf40c3f3",
   "metadata": {},
   "source": [
    "### List Merge Version\n",
    "\n",
    "A better idea might be to use the memory of Spark to make things faster. We are going to make the assumption that each Spark worker can hold two entire friends lists at once.  So this idea here is:\n",
    "* merge pairs of lists representing, e.g. from 100 and 200, into an RDD. You should generate an RDD with entries like:\n",
    "    * `'100, 200', array([300, 319, 400])` from 100  \n",
    "    * `'100, 200', array([219, 300, 400])` from 200\n",
    "    * note that we sorted friends in the key so that the keys will match.\n",
    "    * also note that list only contains the remaining friends, i.e. not 100 or 200\n",
    "* group these lists together\n",
    "    * '100, 200', [array([219, 300, 400]), array([300, 319, 400])]`\n",
    "* when you have two arrays in the value, compute their intersection\n",
    "    * '100, 200', [300, 400]\n",
    "* output the corresponding triples\n",
    "    * be careful here. it's subtle to get the output right.\n",
    "    \n",
    "You have write any helper functions that you need. I'm including prototypes for the ones I used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e7c60-1961-4657-89e1-1ad0038fb77a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Output the remaining list of friends for each friend.\n",
    "def pair_lists(line: str):\n",
    "    fids = np.array(line.split(), dtype=int)\n",
    "    ret = []\n",
    "    for i in range(1, len(fids) - 1):\n",
    "        source = fids[0]\n",
    "        if source < fids[i]:\n",
    "            ret.append([f\"{source}, {fids[i]}\", np.concatenate((fids[1:i], fids[i+1:]),)])\n",
    "        else:\n",
    "            ret.append([f\"{fids[i]}, {source}\", np.concatenate((fids[1:i], fids[i+1:]),)])            \n",
    "    return ret    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c067377a-383d-41e4-a772-3fb09b50a8a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pair_lists (\"2 1 5 8 7 9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a7fb23-6006-41f8-add1-59f8e220573d",
   "metadata": {},
   "source": [
    "Sample output from pair_list\n",
    "```\n",
    "[['1, 2', array([5, 8, 7, 9])],\n",
    " ['2, 5', array([1, 8, 7, 9])],\n",
    " ['2, 8', array([1, 5, 7, 9])],\n",
    " ['2, 7', array([1, 5, 8, 9])]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff86f224-8844-410e-97ef-6777b9110c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Output the appropriate triples after intersection.\n",
    "def output_triples(x):\n",
    "    output = []\n",
    "    xar = np.fromstring(x[0], dtype=int, sep=\",\")\n",
    "    for third in x[1]:\n",
    "        if xar[0] < xar[1]:\n",
    "            output.append((third, xar[0], xar[1]))\n",
    "        else:\n",
    "            output.append((third, xar[1], xar[0]))\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c12197-0508-45d8-a640-db127f40c325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_triples(('100, 200', [300, 400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99bb673-9690-4530-93f3-00e96bf2d09c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46febb59-f3d0-4ba2-a71b-17374d213811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../../data/simple.input\"\n",
    "outdir = \"/tmp/output.merge8\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "rdd = rdd.flatMap(pair_lists)\n",
    "rdd = rdd.groupByKey().mapValues(list)\n",
    "rdd = rdd.filter(lambda x: len(x[1])==2)\n",
    "rdd = rdd.mapValues(lambda x: np.intersect1d(x[0], x[1]))\n",
    "rdd = rdd.flatMap(output_triples)\n",
    "rdd.saveAsTextFile(outdir)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b40b05-8fe1-42db-937e-bc2b5c7ad964",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Timings\n",
    "\n",
    "Run on the full dataset to compare performance.\n",
    "\n",
    "I've removed the code, but I've left the timing information for your reference.\n",
    "  * M/R OK\n",
    "  * Join slowest\n",
    "  * Merge fastest\n",
    "Conclusion: different implementations better for different architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf56ceff-a675-46e6-aef8-6cf1364e7f34",
   "metadata": {},
   "source": [
    "##### Map/Reduce version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9370bc58-8032-4877-a9ac-cd6e9b39aff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../data/fof.input\"\n",
    "outdir = \"/tmp/outputfof.mr\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "rdd = rdd.flatMap(line_to_triples)\n",
    "rdd = rdd.map(lambda x: (str(x), 1))\n",
    "rdd = rdd.reduceByKey(lambda x, y: x + y)\n",
    "rdd = rdd.filter(lambda x: x[1] > 1)\n",
    "rdd = rdd.map(lambda x: x[0])\n",
    "rdd.saveAsTextFile(outdir)\n",
    "\n",
    "sc.stop()                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30103e99-127b-4a1e-b29a-3010cd8cc0f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Join version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b904e2-cf58-49fc-ba45-63205d15742c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../data/fof.input\"\n",
    "outdir = \"/tmp/outputfof.join\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "rdd = rdd.flatMap(line_to_triples)\n",
    "rdd = rdd.mapPartitionsWithIndex(add_index)\n",
    "rdd = rdd.join(rdd)\n",
    "rdd = rdd.map(filter_diff_idx).filter(lambda x: x!= None)\n",
    "rdd.saveAsTextFile(outdir)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfee4c5-1fe6-41ae-ac92-bac4a41b0792",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### List merge version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54c01ad-698f-4f05-ba77-e2582f18f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../data/fof.input\"\n",
    "outdir = \"/tmp/outputfof.merge\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "rdd = rdd.flatMap(pair_lists)\n",
    "rdd = rdd.groupByKey().mapValues(list)\n",
    "rdd = rdd.filter(lambda x: len(x[1])==2)\n",
    "rdd = rdd.mapValues(lambda x: np.intersect1d(x[0], x[1]))\n",
    "rdd = rdd.flatMap(output_triples)\n",
    "rdd.saveAsTextFile(outdir)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9016a66-7fb1-4eee-8be8-4a45242ac468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
