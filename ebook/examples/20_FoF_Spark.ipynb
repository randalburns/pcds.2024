{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce265c4c-a998-41f8-a413-2919cde7edc3",
   "metadata": {},
   "source": [
    "## PySpark Friends of Friends\n",
    "\n",
    "**The notebook is complete, i.e. all the TODOs have been filled in.**\n",
    "\n",
    "We are going to reproduce the Friends of Friends Hadoop example in Spark.\n",
    "First things first, let's run it as a map/reduce program in Spark.\n",
    "\n",
    "Node A with neighbors B and C propose candidate triples to it's neighbors\n",
    "* B A C to node B (A<C, else B C A)\n",
    "* C A B to node C (A<B else C B A)\n",
    "\n",
    "All triples will get two proposals from it's neighbors and reduce them. If there are two matching proposals, we have a triple.\n",
    "\n",
    "### Map/Reduce Implementation\n",
    "\n",
    "The code below `lines_to_triples` implements the proposal to be run in `flatMap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9eca2-47cd-4201-b6ed-6c4c8ce93d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def line_to_triples(line: str):\n",
    "    fids = np.array(line.split(), dtype=int)\n",
    "    ret = []\n",
    "    for i in range(1, len(fids) - 1):\n",
    "        for j in  range(i + 1, len(fids)):\n",
    "            source = fids[0]\n",
    "            fi, fj = fids[i], fids[j]\n",
    "            if source < fi:\n",
    "                ret.append([fj, source, fi])\n",
    "            else:\n",
    "                ret.append([fj, fi, source])\n",
    "            if source < fj:\n",
    "                ret.append([fi, source, fj])\n",
    "            else:\n",
    "                ret.append([fi, fj, source])\n",
    "    return ret    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b704dc7-6fbf-42a2-bc7c-7c7442c62f5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Simple test to show what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3741e37d-be14-4f54-a983-25001937827e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "line_to_triples (\"1 5 8 7 9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f53ec-ed5a-44e5-8562-adf4bfd45975",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a99c61-970d-4cbf-9bb6-c6d9bd599b54",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "Now, complete the program. You will have to use the wordcount style `<triple>, 1` to get a simpler reducer to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e88800-1cf0-485d-af50-046bc37dd4fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../data/simple.input\"\n",
    "outdir = \"/tmp/outputsimple.mr\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "rdd = rdd.flatMap(line_to_triples)\n",
    "### TODO add a WordCount style reducer\n",
    "rdd = rdd.map(lambda x: (str(x), 1))\n",
    "rdd = rdd.reduceByKey(lambda x, y: x + y)\n",
    "### TODO Identify triples of count >=2\n",
    "rdd = rdd.filter(lambda x: x[1]>1)\n",
    "### TODO format output to just triples\n",
    "rdd = rdd.map(lambda x: x[0])\n",
    "rdd.saveAsTextFile(outdir)            \n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debed218-ee52-4613-bddd-1ee9ed84390f",
   "metadata": {},
   "source": [
    "#### Gotchas\n",
    "Things that were confusing in the implementation\n",
    "1. `str(x)` not `x`: `reduceByKey()` needs a field that it can reduce on so tuples do not work.\n",
    "2. Warnings: `Unable to load native-hadoop library for your platform`.  This can be ignored.\n",
    "3. Rerunning in the same output directory\n",
    "    1. Spark needs to create an output directory for each run\n",
    "    2. Running twice will create error on `rdd.saveAsTextFile(outdir)`\n",
    "4. Error on `sc = SparkContext(\"local\", \"App Name\",)`\n",
    "    1. Fix this by running `sc.stop()`\n",
    "    2. We should run the whole snippet in a `try` block and stop on error. But, this is jupyter.\n",
    "5. Debugging with Spark. How to build up a program.\n",
    "    1. I do one rdd transformation as a time and check the output.\n",
    "    2. Then change the directory, add a transformation, and check the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e81ac7-27ec-498f-ac24-c26dc4ee38f2",
   "metadata": {},
   "source": [
    "### Join Implementation\n",
    "\n",
    "In previous years when we ran this on distributed memory, we found that an implementation using a self `join` was faster than `reduceByKey`. This is because join has efficient implementation (based on a hash map) that do not have to send all the triples around. The concept of the program is:\n",
    "* output all triples (same as before)\n",
    "* add an partition number to each triple\n",
    "* perform a self-join on the triples with partition numbers\n",
    "* use the partition index to identify when the triples came from different lists\n",
    "    * self-join will produce joined triples from the same partition \n",
    "* output triples when two proposals came from different partitions\n",
    "    * i.e. two proposal from different friends lists\n",
    "    * you need to avoid additional output when there is no match\n",
    "    \n",
    "This is a pretty different usage of Spark.  The function `mapPartitions` allows the programmer to write functions that apply to an entire partition, rather than each individual element of an RDD.  The function `mapPartitionsWithIndex` makes the partition identifier available to differentiate behavior, analagous to have the thread ID in OpenMP.\n",
    "\n",
    "Partition in this case refers to an input partition.  There will be one per file because that's how `sc.textFile()` works.  So, each friends list is in a different partition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e48f4-7be1-4e9e-8516-6c8e25b1073b",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following helper function will add an index to the triple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f8a9865-d765-4935-ab73-a2d1aecef851",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### add a parition index to each triple.\n",
    "def add_index (idx, part):\n",
    "    for p in part:\n",
    "        yield str(p), str(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68537dc4-8182-4533-960d-ede8f5df6309",
   "metadata": {
    "tags": []
   },
   "source": [
    "You will need to write a function to identify when the joined triples have different indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7638ab0-0b93-4131-87e2-a2931749ba37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_diff_idx (x):\n",
    "    if x[1][0] != x[1][1]:\n",
    "        return x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c33970c5-0107-45cc-b78f-fcd692bf4eba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/14 13:20:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../data/simple.input\"\n",
    "outdir = \"/tmp/output.join\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "rdd = rdd.flatMap(line_to_triples)\n",
    "rdd = rdd.mapPartitionsWithIndex(add_index)\n",
    "# TODO self join\n",
    "rdd = rdd.join(rdd)\n",
    "# TODO filter out matches from same partition\n",
    "rdd = rdd.map(filter_diff_idx)\n",
    "# TODO cleanup output\n",
    "rdd = rdd.filter(lambda x: x!= None)\n",
    "rdd.saveAsTextFile(outdir)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61753d5-ddd1-4dcc-915c-de554e0054a7",
   "metadata": {},
   "source": [
    "The following `sc.stop()` can be called alone to clean up crashed environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b04ef2a6-420d-43d6-ae0c-734a324c03eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f482b323-0976-4e1b-b069-f60b7741a1b0",
   "metadata": {},
   "source": [
    "It turns out (we will see below) that this version is not faster on shared-memory. It does a lot more computation and the reduction in traffic is not as important, because memory is faster than networking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3871b123-cd3b-4619-8f8e-2af4cf40c3f3",
   "metadata": {},
   "source": [
    "### List Merge Version\n",
    "\n",
    "A better idea might be to use the memory of Spark to make things faster. We are going to make the assumption that each Spark worker can hold two entire friends lists at once.  So this idea here is:\n",
    "* merge pairs of lists representing, e.g. from 100 and 200, into an RDD. You should generate an RDD with entries like:\n",
    "    * `'100, 200', array([300, 319, 400])` from 100  \n",
    "    * `'100, 200', array([219, 300, 400])` from 200\n",
    "    * note that we sorted friends in the key so that the keys will match.\n",
    "    * also note that list only contains the remaining friends, i.e. not 100 or 200\n",
    "* group these lists together\n",
    "    * '100, 200', [array([219, 300, 400]), array([300, 319, 400])]`\n",
    "* when you have two arrays in the value, compute their intersection\n",
    "    * '100, 200', [300, 400]\n",
    "* output the corresponding triples\n",
    "    * be careful here. it's subtle to get the output right.\n",
    "    \n",
    "You have write any helper functions that you need. I'm including prototypes for the ones I used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e7c60-1961-4657-89e1-1ad0038fb77a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Output the remaining list of friends for each friend.\n",
    "def pair_lists(line: str):\n",
    "    fids = np.array(line.split(), dtype=int)\n",
    "    ret = []\n",
    "    for i in range(1, len(fids)):\n",
    "        source = fids[0]\n",
    "        if source < fids[i]:\n",
    "            ret.append([f\"{source}, {fids[i]}\", np.concatenate((fids[1:i], fids[i+1:]),)])\n",
    "        else:\n",
    "            ret.append([f\"{fids[i]}, {source}\", np.concatenate((fids[1:i], fids[i+1:]),)])            \n",
    "    return ret    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c067377a-383d-41e4-a772-3fb09b50a8a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pair_lists (\"2 1 5 8 7 9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a7fb23-6006-41f8-add1-59f8e220573d",
   "metadata": {},
   "source": [
    "Sample output from pair_list\n",
    "```\n",
    "[['1, 2', array([5, 8, 7, 9])],\n",
    " ['2, 5', array([1, 8, 7, 9])],\n",
    " ['2, 8', array([1, 5, 7, 9])],\n",
    " ['2, 7', array([1, 5, 8, 9])]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff86f224-8844-410e-97ef-6777b9110c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Output the appropriate triples after intersection.\n",
    "def output_triples(x):\n",
    "    output = []\n",
    "    xar = np.fromstring(x[0], dtype=int, sep=\",\")\n",
    "    for third in x[1]:\n",
    "        if xar[0] < xar[1]:\n",
    "            output.append((third, xar[0], xar[1]))\n",
    "        else:\n",
    "            output.append((third, xar[1], xar[0]))\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99bb673-9690-4530-93f3-00e96bf2d09c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46febb59-f3d0-4ba2-a71b-17374d213811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../data/simple.input\"\n",
    "outdir = \"/tmp/output.mergenew\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "rdd = rdd.flatMap(pair_lists)\n",
    "# group by and turn into lists\n",
    "rdd = rdd.groupByKey().mapValues(list)\n",
    "# keep only keys with two inputs\n",
    "rdd = rdd.filter(lambda x: len(x[1])==2)\n",
    "# TODO intersect the two lists\n",
    "rdd = rdd.mapValues(lambda x: np.intersect1d(x[0], x[1]))\n",
    "rdd = rdd.flatMap(output_triples)\n",
    "rdd.saveAsTextFile(outdir)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b40b05-8fe1-42db-937e-bc2b5c7ad964",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Timings\n",
    "\n",
    "Run on the full dataset to compare performance.\n",
    "\n",
    "I've removed the code, but I've left the timing information for your reference.\n",
    "  * M/R OK\n",
    "  * Join slowest\n",
    "  * Merge fastest\n",
    "Conclusion: different implementations better for different architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf56ceff-a675-46e6-aef8-6cf1364e7f34",
   "metadata": {},
   "source": [
    "#### Map/Reduce version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9370bc58-8032-4877-a9ac-cd6e9b39aff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../data/fof.input\"\n",
    "outdir = \"/tmp/outputfof.mr1\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "rdd = rdd.flatMap(line_to_triples)\n",
    "...\n",
    "rdd.saveAsTextFile(outdir)\n",
    "\n",
    "sc.stop()                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30103e99-127b-4a1e-b29a-3010cd8cc0f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Join version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b904e2-cf58-49fc-ba45-63205d15742c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../data/fof.input\"\n",
    "outdir = \"/tmp/outputfof.joinc\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "rdd = rdd.flatMap(line_to_triples)\n",
    "...\n",
    "rdd.saveAsTextFile(outdir)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfee4c5-1fe6-41ae-ac92-bac4a41b0792",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### List merge version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54c01ad-698f-4f05-ba77-e2582f18f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../data/fof.input\"\n",
    "outdir = \"/tmp/outputfof.merge1\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "...\n",
    "rdd.saveAsTextFile(outdir)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9016a66-7fb1-4eee-8be8-4a45242ac468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
