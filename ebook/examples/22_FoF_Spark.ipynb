{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce265c4c-a998-41f8-a413-2919cde7edc3",
   "metadata": {},
   "source": [
    "## PySpark Friends of Friends\n",
    "\n",
    "We are going to reproduce the Friends of Friends Hadoop example in Spark.\n",
    "First things first, let's run it as a map/reduce program in Spark.\n",
    "\n",
    "Node A with neighbors B and C propose candidate triples to it's neighbors\n",
    "* B A C to node B (A<C, else B C A)\n",
    "* C A B to node C (A<B else C B A)\n",
    "\n",
    "All triples will get two proposals from it's neighbors and reduce them. If there are two matching proposals, we have a triple.\n",
    "\n",
    "#### Map/Reduce Implementation\n",
    "\n",
    "The code below `lines_to_triples` implements the proposal to be run in `flatMap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb9eca2-47cd-4201-b6ed-6c4c8ce93d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def line_to_triples(line: str):\n",
    "    fids = np.array(line.split(), dtype=int)\n",
    "    ret = []\n",
    "    for i in range(1, len(fids) - 1):\n",
    "        for j in  range(i + 1, len(fids)):\n",
    "            source = fids[0]\n",
    "            fi, fj = fids[i], fids[j]\n",
    "            if source < fi:\n",
    "                ret.append([fj, source, fi])\n",
    "            else:\n",
    "                ret.append([fj, fi, source])\n",
    "            if source < fj:\n",
    "                ret.append([fi, source, fj])\n",
    "            else:\n",
    "                ret.append([fi, fj, source])\n",
    "    return ret    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b704dc7-6fbf-42a2-bc7c-7c7442c62f5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Simple test to show what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3741e37d-be14-4f54-a983-25001937827e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 1, 5],\n",
       " [5, 1, 8],\n",
       " [7, 1, 5],\n",
       " [5, 1, 7],\n",
       " [9, 1, 5],\n",
       " [5, 1, 9],\n",
       " [7, 1, 8],\n",
       " [8, 1, 7],\n",
       " [9, 1, 8],\n",
       " [8, 1, 9],\n",
       " [9, 1, 7],\n",
       " [7, 1, 9]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_to_triples (\"1 5 8 7 9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d4f53ec-ed5a-44e5-8562-adf4bfd45975",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msc\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a99c61-970d-4cbf-9bb6-c6d9bd599b54",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "Now, complete the program. You will have to use the wordcount style `<triple>, 1` to get a simpler reducer to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc876d3a-caa7-4ba6-a55c-2869fb36eae4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/15 20:19:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../data/simple.input\"\n",
    "outdir = \"/tmp/outputsimple.mr9\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "rdd = rdd.flatMap(line_to_triples)\n",
    "### TODO add a WordCount style reducer\n",
    "rdd = rdd.map(lambda x: (str(x), 1))\n",
    "rdd = rdd.reduceByKey(lambda x, y: x + y)\n",
    "### TODO Identify triples of count >=2\n",
    "rdd = rdd.filter(lambda x: x[1]>1)\n",
    "### TODO format output to just triples\n",
    "rdd = rdd.map(lambda x: x[0])\n",
    "rdd.saveAsTextFile(outdir)\n",
    "\n",
    "sc.stop()             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1192af-571a-4b2a-a2b6-46f72e7c4190",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e81ac7-27ec-498f-ac24-c26dc4ee38f2",
   "metadata": {},
   "source": [
    "#### Join Implementation\n",
    "\n",
    "In previous years when we ran this on distributed memory, we found that an implementation using a `join` was faster. This is because join has efficient implementation (based on a hash map) that do not have to send all the triples around. The concept of the program is:\n",
    "* output all triples (same as before)\n",
    "* add an partition number to each triple\n",
    "* perform a self-join on the triples with partition numbers\n",
    "* use the partition index to identify when the triples came from different lists\n",
    "    * self-join will produce joined triples from the same partition \n",
    "* output triples when two proposals came from different partitions\n",
    "    * i.e. two proposal from different friends lists\n",
    "    * you need to avoid additional output when there is no match\n",
    "    \n",
    "This is a pretty different usage of Spark.  The function `mapPartitions` allows the programmer to write functions that apply to an entire partition, rather than each individual element of an RDD.  The function `mapPartitionsWithIndex` makes the partition identifier available to differentiate behavior, analagous to have the thread ID in OpenMP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e48f4-7be1-4e9e-8516-6c8e25b1073b",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following helper function will add an index to the triple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f8a9865-d765-4935-ab73-a2d1aecef851",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### add a parition index to each triple.\n",
    "def add_index (idx, part):\n",
    "    for p in part:\n",
    "        yield str(p), str(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68537dc4-8182-4533-960d-ede8f5df6309",
   "metadata": {
    "tags": []
   },
   "source": [
    "You will need to write a function to identify when the joined triples have different indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7638ab0-0b93-4131-87e2-a2931749ba37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_diff_idx (x):\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c33970c5-0107-45cc-b78f-fcd692bf4eba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/07 21:22:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../data/simple.input\"\n",
    "outdir = \"/tmp/output.join\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "rdd = rdd.flatMap(line_to_triples)\n",
    "rdd = rdd.mapPartitionsWithIndex(add_index)\n",
    "# TODO self join\n",
    "...\n",
    "# TODO filter out matches from same partition\n",
    "...\n",
    "# TODO cleanup output\n",
    "...\n",
    "rdd.saveAsTextFile(outdir)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61753d5-ddd1-4dcc-915c-de554e0054a7",
   "metadata": {},
   "source": [
    "The following `sc.stop()` can be called alone to clean up crashed environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b04ef2a6-420d-43d6-ae0c-734a324c03eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f482b323-0976-4e1b-b069-f60b7741a1b0",
   "metadata": {},
   "source": [
    "It turns out (we will see below) that this version is not faster on shared-memory. It does a lot more computation and the reduction in traffic is not as important, because memory is faster than networking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3871b123-cd3b-4619-8f8e-2af4cf40c3f3",
   "metadata": {},
   "source": [
    "### List Merge Version\n",
    "\n",
    "A better idea might be to use the memory of Spark to make things faster. We are going to make the assumption that each Spark worker can hold two entire friends lists at once.  So this idea here is:\n",
    "* merge pairs of lists representing, e.g. from 100 and 200, into an RDD. You should generate an RDD with entries like:\n",
    "    * `'100, 200', array([300, 319, 400])` from 100  \n",
    "    * `'100, 200', array([219, 300, 400])` from 200\n",
    "    * note that we sorted friends in the key so that the keys will match.\n",
    "    * also note that list only contains the remaining friends, i.e. not 100 or 200\n",
    "* group these lists together\n",
    "    * '100, 200', [array([219, 300, 400]), array([300, 319, 400])]`\n",
    "* when you have two arrays in the value, compute their intersection\n",
    "    * '100, 200', [300, 400]\n",
    "* output the corresponding triples\n",
    "    * be careful here. it's subtle to get the output right.\n",
    "    \n",
    "You have write any helper functions that you need. I'm including prototypes for the ones I used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae4e7c60-1961-4657-89e1-1ad0038fb77a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Output the remaining list of friends for each friend.\n",
    "def pair_lists(line: str):\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c067377a-383d-41e4-a772-3fb09b50a8a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1, 2', array([5, 8, 7, 9])],\n",
       " ['2, 5', array([1, 8, 7, 9])],\n",
       " ['2, 8', array([1, 5, 7, 9])],\n",
       " ['2, 7', array([1, 5, 8, 9])]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_lists (\"2 1 5 8 7 9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a7fb23-6006-41f8-add1-59f8e220573d",
   "metadata": {},
   "source": [
    "Sample output from pair_list\n",
    "```\n",
    "[['1, 2', array([5, 8, 7, 9])],\n",
    " ['2, 5', array([1, 8, 7, 9])],\n",
    " ['2, 8', array([1, 5, 7, 9])],\n",
    " ['2, 7', array([1, 5, 8, 9])]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff86f224-8844-410e-97ef-6777b9110c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Output the appropriate triples after intersection.\n",
    "def output_triples(x):\n",
    "    output = []\n",
    "    # TODO\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c99bb673-9690-4530-93f3-00e96bf2d09c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46febb59-f3d0-4ba2-a71b-17374d213811",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../data/simple.input\"\n",
    "outdir = \"/tmp/output.merge\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "rdd = rdd.flatMap(pair_lists)\n",
    "# group by and turn into lists\n",
    "rdd = rdd.groupByKey().mapValues(list)\n",
    "# keep only keys with two inputs\n",
    "rdd = rdd.filter(lambda x: len(x[1])==2)\n",
    "# TODO intersect the two lists\n",
    "...\n",
    "rdd = rdd.flatMap(output_triples)\n",
    "rdd.saveAsTextFile(outdir)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b40b05-8fe1-42db-937e-bc2b5c7ad964",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Timings\n",
    "\n",
    "Run on the full dataset to compare performance.\n",
    "\n",
    "I've removed the code, but I've left the timing information for your reference.\n",
    "  * M/R OK\n",
    "  * Join slowest\n",
    "  * Merge fastest\n",
    "Conclusion: different implementations better for different architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf56ceff-a675-46e6-aef8-6cf1364e7f34",
   "metadata": {},
   "source": [
    "##### Map/Reduce version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9370bc58-8032-4877-a9ac-cd6e9b39aff2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3min 30s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../data/fof.input\"\n",
    "outdir = \"/tmp/outputfof.mr\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "rdd = rdd.flatMap(line_to_triples)\n",
    "...\n",
    "rdd.saveAsTextFile(outdir)\n",
    "\n",
    "sc.stop()                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30103e99-127b-4a1e-b29a-3010cd8cc0f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Join version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3b904e2-cf58-49fc-ba45-63205d15742c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8min 25s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../data/fof.input\"\n",
    "outdir = \"/tmp/outputfof.join\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "rdd = rdd.flatMap(line_to_triples)\n",
    "...\n",
    "rdd.saveAsTextFile(outdir)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfee4c5-1fe6-41ae-ac92-bac4a41b0792",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### List merge version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d54c01ad-698f-4f05-ba77-e2582f18f99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2min 32s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../data/fof.input\"\n",
    "outdir = \"/tmp/outputfof.merge\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "...\n",
    "rdd.saveAsTextFile(outdir)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9016a66-7fb1-4eee-8be8-4a45242ac468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
